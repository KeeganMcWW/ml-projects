{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IASIM2016_Challenege_ML.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "wCLLuckDKhQA"
      ],
      "authorship_tag": "ABX9TyOBfzjXT8IyhNOWPWjCBqa3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KeeganMcWW/ml-projects/blob/main/IASIM2016_Challenege_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64kSOhTdKk7n"
      },
      "source": [
        "# Import and set up the Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zjp-hux6tWJX"
      },
      "source": [
        "import scipy.io as sio\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from google.colab import files\n",
        "from scipy.signal import savgol_filter\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xo2236HxHpAH"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-vZwPE4euhx"
      },
      "source": [
        "## Fetch and store data from Eiganvector.com"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQYx9FUit0Pg",
        "outputId": "fe804110-c7bb-47dc-f320-34ce7e19550c"
      },
      "source": [
        "machine_data_path = r'/content/data_files/'\n",
        "try:\n",
        "    os.mkdir(machine_data_path)\n",
        "except OSError:\n",
        "      print (\"Creation of the directory %s failed\" % machine_data_path)\n",
        "else:\n",
        "    print (\"Successfully created the directory %s \" % machine_data_path)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creation of the directory /content/data_files/ failed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBb_xNVst0NY",
        "outputId": "58a6a4b8-90a3-4a8f-e483-9231b77515b2"
      },
      "source": [
        "!wget https://eigenvector.com/wp-content/uploads/2021/03/IASIM16ChallengeProblem.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-01-12 20:53:28--  https://eigenvector.com/wp-content/uploads/2021/03/IASIM16ChallengeProblem.zip\n",
            "Resolving eigenvector.com (eigenvector.com)... 69.163.163.60, 2607:f298:6:a034::eaf:812c\n",
            "Connecting to eigenvector.com (eigenvector.com)|69.163.163.60|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 448641963 (428M) [application/zip]\n",
            "Saving to: ‘IASIM16ChallengeProblem.zip.1’\n",
            "\n",
            "allengeProblem.zip.  67%[============>       ] 288.60M  21.0MB/s    eta 9s     "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faUUITtOC8HI"
      },
      "source": [
        "!unzip IASIM16ChallengeProblem.zip -d data_files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCLLuckDKhQA"
      },
      "source": [
        "## Helper functions to parse the .mat files for use in python"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtXbSqhntvss"
      },
      "source": [
        "# https://stackoverflow.com/questions/7008608/scipy-io-loadmat-nested-structures-i-e-dictionaries\n",
        "def loadmat(filename):\n",
        "    '''\n",
        "    this function should be called instead of direct spio.loadmat\n",
        "    as it cures the problem of not properly recovering python dictionaries\n",
        "    from mat files. It calls the function check keys to cure all entries\n",
        "    which are still mat-objects\n",
        "    '''\n",
        "    def _check_keys(d):\n",
        "        '''\n",
        "        checks if entries in dictionary are mat-objects. If yes\n",
        "        todict is called to change them to nested dictionaries\n",
        "        '''\n",
        "        for key in d:\n",
        "            if isinstance(d[key], sio.matlab.mio5_params.mat_struct):\n",
        "                d[key] = _todict(d[key])\n",
        "        return d\n",
        "\n",
        "    def _todict(matobj):\n",
        "        '''\n",
        "        A recursive function which constructs from matobjects nested dictionaries\n",
        "        '''\n",
        "        d = {}\n",
        "        for strg in matobj._fieldnames:\n",
        "            elem = matobj.__dict__[strg]\n",
        "            if isinstance(elem, sio.matlab.mio5_params.mat_struct):\n",
        "                d[strg] = _todict(elem)\n",
        "            elif isinstance(elem, np.ndarray):\n",
        "                d[strg] = _tolist(elem)\n",
        "            else:\n",
        "                d[strg] = elem\n",
        "        return d\n",
        "\n",
        "    def _tolist(ndarray):\n",
        "        '''\n",
        "        A recursive function which constructs lists from cellarrays\n",
        "        (which are loaded as numpy ndarrays), recursing into the elements\n",
        "        if they contain matobjects.\n",
        "        '''\n",
        "        elem_list = []\n",
        "        for sub_elem in ndarray:\n",
        "            if isinstance(sub_elem, sio.matlab.mio5_params.mat_struct):\n",
        "                elem_list.append(_todict(sub_elem))\n",
        "            elif isinstance(sub_elem, np.ndarray):\n",
        "                elem_list.append(_tolist(sub_elem))\n",
        "            else:\n",
        "                elem_list.append(sub_elem)\n",
        "        return elem_list\n",
        "    data = sio.loadmat(filename, struct_as_record=False, squeeze_me=True)\n",
        "    return _check_keys(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1ZbJdBKKw5H"
      },
      "source": [
        "#Examining the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRUzOOyifRxI"
      },
      "source": [
        "load the imported data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2jh6SQxC2y7"
      },
      "source": [
        "wheat_gluten_pure = loadmat('/content/data_files/Wheat_Gluten_Pure.mat')\n",
        "melamine_pure = loadmat('/content/data_files/Melamine_Pure.mat')\n",
        "m_200ppm = loadmat('/content/data_files/M_200ppm.mat')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qwl97qgxEwKa"
      },
      "source": [
        "Data is imported with as a dict with nestled dicts. The important dict is the 'z' dictionary, which contains the important information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFUCZjvOD12K"
      },
      "source": [
        "print(wheat_gluten_pure['z'].keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NeGPTXBfdnk"
      },
      "source": [
        "Here we can see the data is imported to the correct size. The hyperspectral image should be 243 by 244 with 229 spectral points (a 243x244x229 HS cube)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkfMe5mZD_41"
      },
      "source": [
        "x_size, y_size = wheat_gluten_pure['z']['imagesize']\n",
        "z_size = np.shape(wheat_gluten_pure['z']['data'])[1]\n",
        "print(x_size, y_size, z_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgeFKXXpFA6m"
      },
      "source": [
        "wheat_gluten_pure_data_mat = np.reshape(wheat_gluten_pure['z']['data'], (x_size, y_size, z_size), order = 'F')\n",
        "melamine_pure_data_mat = np.reshape(melamine_pure['z']['data'], (x_size, y_size, z_size), order = 'F')\n",
        "m_200ppm_data_mat = np.reshape(m_200ppm['z']['data'], (x_size, y_size, z_size), order = 'F')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CkSgA8dSXL6"
      },
      "source": [
        "lets take a look at the image. Here I am just slicing along the spectral (z) axis. Each pixel is just the reflectance intensity at the given wavelength. For example, when z_slice = 0 we are looking at the reflectance instensity at 1120nm for the image. \n",
        "\n",
        "Something important to note is it looks like there is some non uniformity in these \"pure\" samples. This problem of typical for any given sampling problem. We have to be careful that by naively training these \"pure\" spectra we don't create outliers which skew the model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32TcHu7lHVCb",
        "cellView": "form"
      },
      "source": [
        "z_slice = 101 #@param {type: \"slider\", min: 0, max: 228}\n",
        "fig, ax = plt.subplots(1,3, figsize=(10, 10));\n",
        "plt.tight_layout();\n",
        "ax[0].imshow(wheat_gluten_pure_data_mat[:, :, z_slice]);\n",
        "ax[0].set_title('Pure Wheat Gluten');\n",
        "ax[1].imshow(melamine_pure_data_mat[:, :, z_slice]);\n",
        "ax[1].set_title('Pure Melamine');\n",
        "ax[2].imshow(m_200ppm_data_mat[:, :, z_slice]);\n",
        "ax[2].set_title('200ppm of Malamin');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMizz-wUS-JW"
      },
      "source": [
        "No we can slice along one of the image axis (either x or y) to check out the spectral data. Here I am slicing along the x axis. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GNlk-bHFt0Q",
        "cellView": "form"
      },
      "source": [
        "x_slice = 79 #@param {type: \"slider\", min: 0, max: 242}\n",
        "fig, ax = plt.subplots(3,1, figsize=(8,8));\n",
        "plt.tight_layout()\n",
        "ax[0].plot(wheat_gluten_pure_data_mat[x_slice, :, :].T)\n",
        "ax[0].set_title('Pure Wheat Gluten');\n",
        "ax[1].plot(melamine_pure_data_mat[x_slice, :, :].T)\n",
        "ax[1].set_title('Pure Melamine');\n",
        "ax[2].plot(m_200ppm_data_mat[x_slice, :, :].T)\n",
        "ax[2].set_title('200ppm of Malamin');\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Av7eOXtPgQQp"
      },
      "source": [
        "Now we can pull the center(ish) pixel from each of the three training HSIs (Hyper Spectral Images) just to take a look at them. The pure melamine spectrum is pretty prenounced, however the low concentration melamine sample clearly has the backgroup spectrim of thw wheat gluten dominating. This is typical for in \"target detection\", but makes for a challenging problem. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_9WWN5jRrgc"
      },
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.plot(wheat_gluten_pure_data_mat[122, 122, :].T, label = 'Pure Wheat Gluten')\n",
        "ax.plot(melamine_pure_data_mat[122, 122, :].T, label = 'Pure Melamine')\n",
        "ax.plot(m_200ppm_data_mat[122, 122, :].T, label = '200ppm Melamine')\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NbSI494ir55"
      },
      "source": [
        "Right now I am just cutting a 100x100 square out of each image. This really should be optimized for the full shape, but this is quick.\n",
        "\n",
        "We can then view it by again slicing along the spectral axis. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CiZn6EcRrdc"
      },
      "source": [
        "crop_size = 100\n",
        "left = int(np.floor((x_size - crop_size)/2))\n",
        "top = int(np.floor((y_size - crop_size)/2))\n",
        "right = int(np.floor((x_size + crop_size)/2))\n",
        "bottom = int(np.floor((y_size + crop_size)/2))\n",
        "\n",
        "wheat_cropped = wheat_gluten_pure_data_mat[left:right, top:bottom, :]\n",
        "melamine_cropped = melamine_pure_data_mat[left:right, top:bottom, :]\n",
        "m_200ppm_cropped = m_200ppm_data_mat[left:right, top:bottom, :]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPD2B79IRrau",
        "cellView": "form"
      },
      "source": [
        "z_slice = 34 #@param {type: \"slider\", min: 0, max: 242}\n",
        "fig, ax = plt.subplots(1,3, figsize=(10, 10));\n",
        "plt.tight_layout();\n",
        "ax[0].imshow(wheat_cropped[:, :, z_slice]);\n",
        "ax[0].set_title('Pure Wheat Gluten');\n",
        "ax[1].imshow(melamine_cropped[:, :, z_slice]);\n",
        "ax[1].set_title('Pure Melamine');\n",
        "ax[2].imshow(m_200ppm_cropped[:, :, z_slice]);\n",
        "ax[2].set_title('200ppm of Malamin');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xl01UsY7kZ4x"
      },
      "source": [
        "# Preprocessing the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krk_qpjOkdO9"
      },
      "source": [
        "## Savitzky-Golay filter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5-oCc4Z4ljj"
      },
      "source": [
        "def apply_sgfilter(matrix, window_length, polyorder, deriv):\n",
        "  output = np.empty(np.shape(matrix))\n",
        "  for i in range(np.shape(matrix)[0]):\n",
        "    for j in range(np.shape(matrix)[1]):\n",
        "      output[i][j] = savgol_filter(matrix[i, j, :], window_length, polyorder, deriv=deriv)\n",
        "      #output[i][j] = np.gradient(sg_filter)\n",
        "  return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Edf7cbZABTBO"
      },
      "source": [
        "First, we are going to test a few smoothing parameters. We will be testing a few different window size/polynomial ratios. The NIRPyResearch articles below give a very good explination of this process, the code is mainly inspired by (or copied from) there."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6O-eI2G4vWK"
      },
      "source": [
        "# https://nirpyresearch.com/choosing-optimal-parameters-savitzky-golay-smoothing-filter/\n",
        "# https://nirpyresearch.com/savitzky-golay-smoothing-method/\n",
        "# Set some reasonable parameters to start with\n",
        "w = 5\n",
        "p = 2\n",
        "# Calculate three different smoothed spectra\n",
        "# first derivative\n",
        "melamine_sgfilter_1_d1 = apply_sgfilter(melamine_cropped, w, polyorder = p, deriv=1)\n",
        "melamine_sgfilter_2_d1 = apply_sgfilter(melamine_cropped, 2*w+1, polyorder = p, deriv=1)\n",
        "melamine_sgfilter_3_d1 = apply_sgfilter(melamine_cropped, 4*w+1, polyorder = 3*p, deriv=1)\n",
        "# second derivative\n",
        "melamine_sgfilter_1_d2 = apply_sgfilter(melamine_cropped, w, polyorder = p, deriv=2)\n",
        "melamine_sgfilter_2_d2 = apply_sgfilter(melamine_cropped, 2*w+1, polyorder = p, deriv=2)\n",
        "melamine_sgfilter_3_d3 = apply_sgfilter(melamine_cropped, 4*w+1, polyorder = 3*p, deriv=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbPZ1xPQ4vSE"
      },
      "source": [
        "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(8,4))\n",
        "plt.tight_layout()\n",
        "axs[0].plot(melamine_sgfilter_1_d1[50, :, :][0], label = 'Smoothing: w/p = 2.5')\n",
        "axs[0].plot(melamine_sgfilter_2_d1[50, :, :][0], label = 'Smoothing: w/p = 5.5')\n",
        "axs[0].plot(melamine_sgfilter_3_d1[50, :, :][0], label = 'Smoothing: w/p = 3.5')\n",
        "axs[0].set_title('Pure Melamine, First Derivative');\n",
        "axs[1].plot(melamine_sgfilter_1_d2[50, :, :][0], label = 'Smoothing: w/p = 2.5')\n",
        "axs[1].plot(melamine_sgfilter_2_d2[50, :, :][0], label = 'Smoothing: w/p = 5.5')\n",
        "axs[1].plot(melamine_sgfilter_3_d3[50, :, :][0], label = 'Smoothing: w/p = 3.5')\n",
        "axs[1].set_title('Pure Melamine, Second Derivative');\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LgaFKB1B6qw"
      },
      "source": [
        "wheat_preprocessed = apply_sgfilter(wheat_cropped, window_length = w, polyorder = p, deriv=2)\n",
        "meamine_preprocessed = apply_sgfilter(melamine_cropped, window_length = w, polyorder = p, deriv=2)\n",
        "m_200ppm_preprocessed = apply_sgfilter(m_200ppm_cropped, window_length = w, polyorder = p, deriv=2)\n",
        "dataset_preprocessing = np.concatenate((wheat_preprocessed, meamine_preprocessed, m_200ppm_preprocessed))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtBmNvJzCU9F",
        "cellView": "form"
      },
      "source": [
        "x_slice = 43 #@param {type: \"slider\", min: 0, max: 99}\n",
        "fig, axs = plt.subplots(3, 2, figsize=(8,8))\n",
        "plt.tight_layout()\n",
        "axs[0, 0].plot(wheat_cropped[x_slice, :, :].T)\n",
        "axs[0, 0].set_title('Pure Wheat Gluten');\n",
        "axs[1, 0].plot(melamine_cropped[x_slice, :, :].T)\n",
        "axs[1, 0].set_title('Pure Melamine');\n",
        "axs[2, 0].plot(m_200ppm_cropped[x_slice, :, :].T)\n",
        "axs[2, 0].set_title('200ppm of Malamin');\n",
        "axs[0, 1].plot(wheat_preprocessed[x_slice, :, :].T)\n",
        "axs[0, 1].set_title('Pure Wheat Gluten');\n",
        "axs[1, 1].plot(meamine_preprocessed[x_slice, :, :].T)\n",
        "axs[1, 1].set_title('Pure Melamine');\n",
        "axs[2, 1].plot(m_200ppm_preprocessed[x_slice, :, :].T)\n",
        "axs[2, 1].set_title('200ppm of Malamin');\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xeJshqXkqSU"
      },
      "source": [
        "Here I am going to great a few labels for a supervised learning algorthem. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRf-AscUCrCi"
      },
      "source": [
        "def create_label(matrix, label):\n",
        "  output = np.empty(np.shape(matrix[:,:,-1]))\n",
        "  for i in range(np.shape(matrix)[0]):\n",
        "    for j in range(np.shape(matrix)[1]):\n",
        "      output[i][j] = label\n",
        "  return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wheat_labels = create_label(wheat_cropped, 1)\n",
        "melamine_labels = create_label(melamine_cropped, 0)\n",
        "#m_200ppm_labels = create_label(m_200ppm_preprocessed, 200/1000000)\n",
        "m_200ppm_labels = create_label(m_200ppm_cropped, 0)\n",
        "wheat_labels_flat = np.reshape(wheat_labels, (crop_size * crop_size), order = 'F')\n",
        "melamine_labels_flat = np.reshape(melamine_labels, (crop_size * crop_size), order = 'F')\n",
        "m_200ppm_labels_flat = np.reshape(m_200ppm_labels, (crop_size * crop_size), order = 'F')"
      ],
      "metadata": {
        "id": "HcRitL7KyCHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wheat_flat = np.reshape(wheat_cropped, (crop_size * crop_size, z_size), order = 'F')\n",
        "melamine_flat = np.reshape(melamine_cropped, (crop_size * crop_size, z_size), order = 'F')\n",
        "m_200ppm_flat = np.reshape(m_200ppm_cropped, (crop_size * crop_size, z_size), order = 'F')"
      ],
      "metadata": {
        "id": "Bbi53DHo3T0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = np.concatenate((wheat_flat, melamine_flat, m_200ppm_flat))\n",
        "labels = np.concatenate((wheat_labels_flat, melamine_labels_flat, m_200ppm_labels_flat))"
      ],
      "metadata": {
        "id": "l6MEBcQXx_Do"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWtRo0AVNIqR"
      },
      "source": [
        "wheat_preprocessed_labels = create_label(wheat_preprocessed, 1)\n",
        "melamine_preprocessed_labels = create_label(meamine_preprocessed, 0)\n",
        "#m_200ppm_labels = create_label(m_200ppm_preprocessed, 200/1000000)\n",
        "m_200ppm_preprocessed_labels = create_label(m_200ppm_preprocessed, 0)\n",
        "wheat_preprocessed_labels_flat = np.reshape(wheat_preprocessed_labels, (crop_size * crop_size), order = 'F')\n",
        "melamine_preprocessed_labels_flat = np.reshape(melamine_preprocessed_labels, (crop_size * crop_size), order = 'F')\n",
        "m_200ppm_preprocessed_labels_flat = np.reshape(m_200ppm_preprocessed_labels, (crop_size * crop_size), order = 'F')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_F3B5TKO3zW"
      },
      "source": [
        "wheat_preprocessed_flat = np.reshape(wheat_preprocessed, (crop_size * crop_size, z_size), order = 'F')\n",
        "meamine_preprocessed_flat = np.reshape(meamine_preprocessed, (crop_size * crop_size, z_size), order = 'F')\n",
        "m_200ppm_preprocessed_flat = np.reshape(m_200ppm_preprocessed, (crop_size * crop_size, z_size), order = 'F')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_preprocessed = np.concatenate((wheat_preprocessed_flat, meamine_preprocessed_flat, m_200ppm_preprocessed_flat))\n",
        "labels_preprocessed = np.concatenate((wheat_preprocessed_labels_flat, melamine_preprocessed_labels_flat, m_200ppm_preprocessed_labels_flat))"
      ],
      "metadata": {
        "id": "KriIVdYHy78u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LogIy3JURVBE"
      },
      "source": [
        "print(np.shape(dataset))\n",
        "print(np.shape(labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.shape(dataset_preprocessed))\n",
        "print(np.shape(labels_preprocessed))"
      ],
      "metadata": {
        "id": "yOvRnxzQzSOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEQIMr5Tk1UR"
      },
      "source": [
        "## PCA Decomposition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VT89Kp58k9vt"
      },
      "source": [
        "\n",
        "Here I am letting the PCA algorthem decompose the spectrum in some number of principle components based on some describable variance criteria. That is to say, n compoenents will be solved until some percetage of variance is explained by the PCA decompisition.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytwZ4uv7J9UH"
      },
      "source": [
        "pca = PCA(n_components=0.98, svd_solver = 'full')\n",
        "pca.fit(dataset)\n",
        "print(pca)\n",
        "print('The number of components is: ' + str(len(pca.explained_variance_ratio_)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca_preprocessed = PCA(n_components=0.98, svd_solver = 'full')\n",
        "pca_preprocessed.fit(dataset_preprocessed)\n",
        "print(pca_preprocessed)\n",
        "print('The number of components is: ' + str(len(pca_preprocessed.explained_variance_ratio_)))"
      ],
      "metadata": {
        "id": "bFaH6zV3zMOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQwVYEvKoMj4"
      },
      "source": [
        "We can graph the variance explained by each additional PC. for this problem a large amount if variance can be explained by a single PC. This is typical for spectral data since it is highly correlated. \n",
        "\n",
        "This is handy since now we have a means to transform our large number of corrilated spectral points into a number of uncorrilated features (in the form of orthogonal PCs). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4pF1mN7Lain"
      },
      "source": [
        "PCs_to_view = 10\n",
        "fig, ax = plt.subplots()\n",
        "plt.plot(pca.explained_variance_ratio_)\n",
        "ax.set(xlabel='Princple Components', ylabel='Explained Variance',)\n",
        "plt.tight_layout();\n",
        "ax.grid()\n",
        "ax.xaxis\n",
        "for x,y in zip(range(PCs_to_view),pca.explained_variance_ratio_):\n",
        "\n",
        "    label = \"{:.3f}\".format(y)\n",
        "\n",
        "    plt.annotate(label, # this is the text\n",
        "                 (x,y), # these are the coordinates to position the label\n",
        "                 textcoords=\"offset points\", # how to position the text\n",
        "                 xytext=(0,3), # distance from text to points (x,y)\n",
        "                 ha='left') # horizontal alignment can be left, right or center\n",
        "plt.xlim(0,PCs_to_view)\n",
        "plt.title('Explained Variance per Principle Component')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSn966y3p1fU"
      },
      "source": [
        "# Data Pipeline and ML Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UHIOfNFqryg"
      },
      "source": [
        "I am going to give a SVM a shot at labeling outliers. The idea is that the spectral preprocessing and decompisiton give the SVM the oppertunity to be able to descriminate the low concetration melamine spectra from background wheat gluten. \n",
        "\n",
        "We can tune the C parameter with a hyperparameter tuner, we well as play with other options like the kernal type. At this point it is worth pointing out we have other parameters to consider as well (for example the SG Filters parameters). We will start with the model parameters to keep things simple. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5AZupFCOa1C"
      },
      "source": [
        "model = svm.SVC()\n",
        "#model.fit(dataset_transformed, labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_Lr-XJxr-jC"
      },
      "source": [
        "I want to try to see how the stock sklearn SVM does on both the preprocessed data  as well as the unprocessed data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8WTEoY8SM29"
      },
      "source": [
        "#%%timeit\n",
        "# the timeit reports a value of ~5 min 20 sec. for the unpreprocessed data\n",
        "cv = KFold(n_splits=5, random_state=1, shuffle=True)\n",
        "scores = cross_val_score(model, dataset, labels, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "print('Accuracy: %.3f (%.3f)' % (np.mean(scores), np.std(scores)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%%timeit\n",
        "# the timeit reports a value of ~5 min 20 sec. for the unpreprocessed data\n",
        "cv = KFold(n_splits=5, random_state=1, shuffle=True)\n",
        "scores = cross_val_score(model, dataset_preprocessed, labels_preprocessed, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "print('Accuracy: %.3f (%.3f)' % (np.mean(scores), np.std(scores)))"
      ],
      "metadata": {
        "id": "kh3_p0iRs0yh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can train an actual svm for use on one of the test examples. "
      ],
      "metadata": {
        "id": "0ZJ_p1IP6kmB"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJ-toQK7Q6eN"
      },
      "source": [
        "val_split = 0.3\n",
        "X_train, X_test, y_train, y_test = train_test_split(dataset_preprocessed, labels_preprocessed, test_size=val_split, random_state=0, shuffle = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrBjnkhPWjse"
      },
      "source": [
        "model.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZW2h-I2BUD9I"
      },
      "source": [
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeNTeoEEV1y7"
      },
      "source": [
        "def predict(matrix, model):\n",
        "  output = np.empty(np.shape(matrix[:,:,-1])) \n",
        "  preprocessed = apply_sgfilter(matrix, w, p, 1)\n",
        "  for i in range(np.shape(matrix)[0]):\n",
        "    for j in range(np.shape(matrix)[1]):\n",
        "      #pca_transform = pca.transform(preprocessed[i,j])\n",
        "      output[i][j] = model.predict(preprocessed[i,j].reshape(1, -1))\n",
        "  return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9hU7ZpYWBlF"
      },
      "source": [
        "test_1 = loadmat('/content/data_files/Test_1.mat')\n",
        "test_1_mat = np.reshape(test_1['z']['data'], (x_size, y_size, z_size), order = 'F')\n",
        "test_1_predict = predict(test_1_mat, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6OkpgJ-Wnzy"
      },
      "source": [
        "plt.imshow(test_1_predict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_2 = loadmat('/content/data_files/Test_2.mat')\n",
        "test_2_mat = np.reshape(test_2['z']['data'], (x_size, y_size, z_size), order = 'F')\n",
        "test_2_predict = predict(test_2_mat, model)"
      ],
      "metadata": {
        "id": "G7IE2e1CAb5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(test_2_predict)"
      ],
      "metadata": {
        "id": "PiDjvowYBXbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_check = loadmat('/content/data_files/Wheat_Gluten_Pure.mat')\n",
        "train_check_mat = np.reshape(train_check['z']['data'], (x_size, y_size, z_size), order = 'F')\n",
        "train_check_mat_predict = predict(train_check_mat, model)"
      ],
      "metadata": {
        "id": "rbSlbDX6BrP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(train_check_mat_predict)"
      ],
      "metadata": {
        "id": "HVXc-k42DpJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "yuybWROJEGer"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}